本文将介绍随机变量的期望、方差等数字特征。

## 期望

### 定义

#### 离散型随机变量

设离散型随机变量 $X$ 的概率分布为 $p_i = P\{ X = x_i \}$，若和式

$$
\sum x_i p_i
$$

绝对收敛，则称其值为 $X$ 的 **期望**，记作 $EX$。

#### 连续型随机变量

设连续型随机变量 $X$ 的密度函数为 $f(x)$。若积分

$$
\int_{\mathbb{R}} xf(x) \text{d} x
$$

绝对收敛，则称其值为 $X$ 的 **期望**，记作 $EX$。

#### 统一定义

设随机变量 $X$ 的分布函数为 $F(x)$，若 [Stieltjes 积分](https://baike.baidu.com/item/%E9%BB%8E%E6%9B%BC%EF%BC%8D%E6%96%AF%E8%92%82%E5%B0%94%E6%9D%B0%E6%96%AF%E7%A7%AF%E5%88%86/15572209)

$$
\int_{\mathbb{R}} x \text{d} F(x)
$$

绝对收敛，则称其值为 $X$ 的 **期望**，记作 $EX$。

!!! example "期望不存在的例子"
    考虑有如下分布的离散型随机变量 $X$
    
    $$
    P\left\{ X = (-1)^k \frac{2^k}{k} \right\} = \frac{1}{2^k}, \quad k = 1, 2, \cdots
    $$
    
    尽管和式 $\sum x_i p_i$ 收敛于 $- \ln 2$，但由于其不是绝对收敛的，故 $X$ 的期望不存在。
    
    再考虑有如下密度函数的连续型随机变量 $Y$
    
    $$
    f(y) = \frac{1}{\pi} \cdot \frac{1}{1 + y^2}, \quad y \in (-\infty, +\infty)
    $$
    
    容易验证 $Y$ 的期望也不存在。

### 期望的性质

#### 线性性

若随机变量 $X, Y$ 的期望存在，则

- 对任意实数 $a, b$，有 $E(aX + b) = a \cdot EX + b$。
- $E(X + Y) = EX + EY$。

#### 随机变量乘积的期望

若随机变量 $X$,$Y$ 的期望存在且 $X$,$Y$ 相互独立，则有

$$
E(XY) = EX \cdot EY
$$

注意：上述性质中的独立性 **并非** 必要条件。

??? example "反例"
    考察随机变量 $X$ 和 $Y$，其中 $X$ 服从 $[-1, 1]$ 上的均匀分布，$Y = X^2$。

### 期望与概率的转化

对于随机事件 $A$，考虑其示性函数 $I_A$：

$$
I_A(\omega) = \begin{cases}
    1, & \omega \in A \\
    0, & \omega \notin A
\end{cases}
$$

根据定义可以求得其期望 $EI_A = P(A)$。这一转化在实际应用中非常常见。

!!! example 例子
    假设对于一个长为 $n$ 的序列 $\{ a_i \}$，其中 $a_k$ 以 $p_k$ 的概率取 $k$，以 $1 - p_k$ 的概率取 $0$。考虑如何求 $S = \sum_{i=1}^{n} a_i$ 的期望。
    
    如果使用定义直接求，需要求出 $S$ 在每个可能取值处的概率，这个计算过程比较繁琐，这里不展开叙述。
    
    另一方面，用 $I_k$ 表示随机事件 $a_k = k$ 的示性函数，则有
    
    $$
    S = \sum_{k=1}^{n} k \cdot I_k
    $$
    
    进而不难求出
    
    $$
    ES = E \left( \sum_{k=1}^{n} k \cdot I_k \right) = \sum_{k=1}^{n} k \cdot E[I_k] = \sum_{k=1}^{n} k \cdot p_k
    $$

## 条件分布与条件期望

我们之前研究过条件概率，类似的也可以提出所谓条件期望的概念。

### 定义

对于两个随机变量 $X$,$Y$，在已知 $Y = y$ 的条件下 $X$ 的概率分布（密度函数）称之为 **条件概率分布（条件概率密度）**，分别记作

$$
P( X = x_i | Y = y ) \qquad f_{X|Y}(x|y)
$$

在此条件下，$X$ 的期望称为 **条件期望**，记作 $E[X|Y=y]$。

### 条件期望的性质

条件期望的诸多性质可由条件概率推知，在此不做赘述。

值得一提的是 $E[X | Y]$ 一般是随机变量 $Y$ 的函数，且这个函数通常不是线性的。但实际上有

$$
E[E[X|Y]] = EX
$$

上式称作 **全期望公式**。

### 应用

例题：[\[LA 7736\]Pocky](https://icpcarchive.ecs.baylor.edu/index.php?option=com_onlinejudge&Itemid=8&category=767&page=show_problem&problem=5758)

题意：有一根长为 $L$ 的 Pocky，每次随机折成两段。若右边一段的长度不大于 $d$ 则停止，否则对右边一段重复上述过程。求重复次数的期望。

??? note "题解"
    记 $f(x)$ 表示长度为 $x$ 的期望次数。$x \leq d$ 的情形平凡。
    
    当 $x > d$ 时，不妨设折断的位置距右端的长度为 $k$，则显然 $k \sim U[0, x]$，此时期望的重复次数为
    
    $$
    g(k) = \begin{cases}
        1, & k \leq d \\
        1 + f(k), & k > d
    \end{cases}
    $$
    
    由全期望公式可知
    
    $$
    f(x) = Eg(k) = 1 + \frac{1}{x} \cdot \int_{d}^{x} f(t) \text{d} t
    $$
    
    解上述积分方程并代入初值条件得
    
    $$
    f(x) = 1 + \ln \frac{x}{d}
    $$

## 方差

### 定义

设随机变量 $X$ 的期望 $EX$ 存在且期望

$$
E(X - EX)^2
$$

也存在，则称上式的值为随机变量 $X$ 的 **方差**，记作 $DX$ 或 $Var(x)$。方差的算术平方根称为 **标准差**，记作 $\sigma(X) = \sqrt{DX}$。

### 方差的性质

若随机变量 $X$ 的方差存在，则

- 对任意常数 $a, b$ 都有 $D(aX + b) = a^2 \cdot DX$
- $DX = E(X^2) - (EX)^2$

## 协方差与相关系数

一般来说，等式 $D(X + Y) = DX + DY$ 并不成立，我们自然会提出两个问题：

- $D(X + Y)$ 与 $DX + DY$ 之间相差的部分到底是什么。
- $D(X + Y)$ 与 $DX + DY$ 在什么情况下相等。

对于第一个问题，我们引入协方差作为解答。

### 协方差的定义

对于随机变量 $X, Y$，称

$$
E((X - EX)(Y - EY))
$$

为 $X$ 与 $Y$ 的 **协方差**，记作 $\operatorname{Cov}(X, Y)$。

### 协方差的性质

对于随机变量 $X, Y, Z$，有

- $\operatorname{Cov}(X, Y) = \operatorname{Cov}(Y, X)$
- 对任意常数 $a, b$，有 $\operatorname{Cov}(aX + bY, Z) = a \cdot \operatorname{Cov}(X, Z) + b \cdot \operatorname{Cov}(Y, Z)$

同时协方差与方差也有如下联系：

- $DX = \operatorname{Cov}(X, X)$
- $D(X + Y) = DX + 2 \operatorname{Cov}(X, Y) + DY$

!!! note "关于协方差"
    你可能会发现协方差的性质与向量内积的运算性质在形式上高度一致。
    
    在泛函分析的视角下，对于给定的概率空间，其上的全体随机变量构成一个线性空间，而协方差是这个空间上的一个内积，标准差则是由该内积导出的范数。

对于刚才提出的第二个问题，不难看出 $D(X + Y) = DX + DY$ 当且仅当 $\operatorname{Cov}(X, Y) = 0$。一个直观的必要条件是 $X$ 与 $Y$ 独立，因为此时有

$$
\operatorname{Cov}(X, Y) = E((X - EX)(Y - EY)) = E(X - EX) E(Y - EY) = 0
$$

但这个条件并不是充分的。为了描述满足 $\operatorname{Cov}(X, Y) = 0$ 的随机变量 $X$,$Y$ 之间的关系，我们引入相关系数

### 相关系数

对于随机变量 $X, Y$，称

$$
\frac{ \operatorname{Cov}(X, Y)}{ \sigma(X)\sigma(Y) }
$$

为 $X$ 与 $Y$ 的 **Pearson 相关系数**，记作 $\rho_{X,Y}$。

Pearson 相关系数描述了两个随机变量之间线性关联的紧密程度。$|\rho_{X,Y}|$ 越大，则 $X$ 与 $Y$ 之间的线性关联程度越强。不难证明 $|\rho_{X,Y}| \leq 1$，且 $|\rho_{X,Y}| = 1$ 仅可能出现在以下两种情况

- 当存在实数 $a$ 和正实数 $b$ 使得 $P(X = a + bY) = 1$ 时，有 $\rho_{X,Y} = 1$；
- 当存在实数 $a$ 和负实数 $b$ 使得 $P(X = a + bY) = 1$ 时，有 $\rho_{X,Y} = -1$。

当 $\rho_{X,Y} = 0$ 时我们称随机变量 $X$ 与 $Y$  **不相关**，此时 $X$ 和 $Y$ 之间不存在线性关系。

!!! note "「不相关」与「独立」"
    两随机变量不相关只是表明他们之间没有线性关联，并不代表没有其他形式的联系。
    
    因此两随机变量 $X, Y$ 不相关是他们相互独立的 **必要而不充分** 条件。

对于这一小节开头提到的第二个问题，我们给出结论：$\operatorname{Cov}(X, Y) = 0$ 的充要条件就是 $X$,$Y$ 中的某一个以概率 $1$ 取常值，或 $X, Y$ 不相关。
