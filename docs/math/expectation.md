## 概述

本文将简要介绍一些概率论的基础概念，并列举一些在算法竞赛中常用的结论，帮助你在短时间内建立起对概率论的直观理解。

为了简单起见，本文在讨论时默认样本空间为 **有限集**。如想了解更为一般的概率理论是如何建立起来的，请阅读概率论章节的其他文章。

## 事件

### 样本空间、随机事件

一个随机现象中可能发生的不能再细分的结果被称为 **样本点**。所有样本点的集合称为 **样本空间**，通常用 $\Omega$ 来表示。

一个 **随机事件** 是样本空间 $\Omega$ 的子集，它由若干样本点构成，用大写字母 $A, B, C, \cdots$ 表示。

对于一个随机现象的结果 $\omega$ 和一个随机事件 $A$，我们称事件 $A$  **发生了** 当且仅当 $\omega \in A$。

例如，掷一次骰子得到的点数是一个随机现象，其样本空间可以表示为 $\Omega=\{1,2,3,4,5,6\}$。设随机事件 $A$ 为“获得的点数大于 $4$”，则 $A = \{ 5, 6 \}$。若某次掷骰子得到的点数 $\omega = 3$，由于 $\omega \notin A$，故事件 $A$ 没有发生。

### 事件的运算

由于我们将随机事件定义为了样本空间 $\Omega$ 的子集，故我们可以将集合的运算（如交、并、补等）移植到随机事件上。记号与集合运算保持一致。

特别的，事件的并 $A \cup B$ 也可记作 $A + B$，事件的交 $A \cap B$ 也可记作 $AB$，此时也可分别称作 **和事件** 和 **积事件**。

## 概率

### 定义

#### 古典定义

如果一个试验满足两条：

- 试验只有有限个基本结果；
- 试验的每个基本结果出现的可能性是一样的；

这样的试验便是古典试验。
对于古典试验中的事件 $A$，它的概率定义为 $P(A)=\frac{m}{n}$，其中 $n$ 表示该试验中所有可能出现的基本结果的总数目，$m$ 表示事件 $A$ 包含的试验基本结果数。

#### 统计定义

如果在一定条件下，进行了 $n$ 次试验，事件 $A$ 发生了 $N_A$ 次，如果随着 $n$ 逐渐增大，频率 $\frac{N_A}{n}$ 逐渐稳定在某一数值 $p$ 附近，那么数值 $p$ 称为事件 $A$ 在该条件下发生的概率，记做 $P(A)=p$。

### 计算

- **广义加法公式**: 对任意两个事件 $A,B$，$P(A \cup B)=P(A)+P(B)-P(A\cap B)$
- **条件概率**: 记 $P(B|A)$ 表示在 $A$ 事件发生的前提下，$B$ 事件发生的概率，则 $P(B|A)=\dfrac{P(AB)}{P(A)}$（其中 $P(AB)$ 为事件 $A$ 和事件 $B$ 同时发生的概率）。
- **乘法公式**:$P(AB)=P(A)\cdot P(B|A)=P(B)\cdot P(A|B)$
- **全概率公式**：若事件 $A_1,A_2,\ldots,A_n$ 构成一组完备的事件且都有正概率，即 $\forall i,j, A_i\cap A_j=\varnothing$ 且 $\displaystyle \sum_{i=1}^n A_i=1$，则有 $\displaystyle P(B)=\sum_{i=1}^n P(A_i)P(B|A_i)$。
- **贝叶斯定理**：$\displaystyle P(B_i|A)=\frac{P(B_i)P(A|B_i)}{\displaystyle \sum_{j=1}^n P(B_j)P(A|B_j)}$

## 随机变量

直观地说，一个随机变量，是一个取值由随机事件决定的变量，“随机变量 $X$ 取值 $\alpha$”（简记为 $X=\alpha$）对应着一个能实现该命题的随机事件，进而有与之对应的概率 $P(X=\alpha)$。

## 独立性

直观地说，我们认为两个东西独立，当它们在某种意义上互不影响。例如，一个人出生的年月日和他的性别，这两件事是独立的；但一个人出生的年月日和他现在的头发总量，这两件事就不是独立的，因为一个人往往年纪越大头发越少。

数学中的独立性与这种直观理解大体相似，但不尽相同。

### 随机事件的独立性

我们称两个事件 $A,B$  **独立**，当 $P(A\cap B)=P(A)P(B)$。

我们称若干个事件 $A_{1\ldots n}$ **互相独立**，当对于其中任何一个子集，该子集中的事件同时发生的概率，等于其中每个事件发生概率的乘积。形式化地说：

$$
P\Big(\bigcap\limits_{E\in T} E\Big)=\prod_{E\in T} P(E), \forall T\subseteq \{A_1,A_2,\ldots,A_n\}
$$

由此可见，若干事件 **两两独立** 和 **互相独立** 是不同的概念。请注意这一点。

### 随机变量的独立性

以下用 $I(X)$ 表示随机变量 $X$ 的取值范围。即，如果把 $X$ 看作一个映射，则 $I(X)$ 就是其值域。

我们称两个随机变量 $X,Y$  **独立**，当 $P\big((X=\alpha)\cap(Y=\beta)\big)=P(X=\alpha)P(Y=\beta),\forall \alpha\in I(X),\beta\in I(Y)$，即 $(X,Y)$ 取任意一组值的概率，等于 $X$ 和 $Y$ 分别取对应值的概率乘积。

我们称若干个随机变量 $X_{1\ldots n}$ **互相独立**，当 $(X_1,\ldots,X_n)$ 取任意一组值的概率，等于每个 $X_i$ 分别取对应值的概率乘积。形式化地说：

$$
P\Big(\bigcap\limits_{i=1}^n X_i=F_i\Big)=\prod\limits_{i=1}^n P(X_i=F_i),\forall F_{1\ldots n} \text{ s.t. } F_i\in I(X_i)
$$

由此可见，若干随机变量 **两两独立** 和 **互相独立** 是不同的概念。请注意这一点。

## 期望

### 定义

如果一个随机变量的取值个数有限（比如一个表示骰子示数的随机变量），或可能的取值可以一一列举出来（比如取值范围为全体正整数），则它称为 **离散型随机变量**。

形式化地说，一个随机变量被称为离散型随机变量，当它的值域大小 **有限** 或者为 **可列无穷大**。

一个离散型随机变量 $X$ 的 **数学期望** 是其每个取值乘以该取值对应概率的总和，记为 $E(X)$。

$$
E(X)=\sum\limits_{\alpha \in I(X)} \alpha\cdot P(X=\alpha)=\sum\limits_{\omega\in S}X(\omega)P(\omega)
$$

其中 $I(X)$ 表示随机变量 $X$ 的值域，$S$ 表示 $X$ 所在概率空间的样本集合。

请读者自行验证连等式中的第二个等号。

### 性质

- **全期望公式**：$E(Y)=\sum\limits_{\alpha \in I(X)} P(X=\alpha)E(Y|(X=\alpha))$，其中 $X,Y$ 是随机变量，$E(Y|A)$ 是在 $A$ 成立的条件下 $Y$ 的期望（即“条件期望”）。可由全概率公式证明。
- **期望的线性性**: 对于任意两个随机变量 $X,Y$（**不要求相互独立**），有 $E(X+Y)=E(X)+E(Y)$。利用这个性质，可以将一个变量拆分成若干个互相独立的变量，分别求这些变量的期望值，最后相加得到所求变量的值。
- **乘积的期望**: 当两个随机变量 $X,Y$ 相互独立时，有 $E(XY)=E(X)E(Y)$。

## 例题

[NOIP2017 初赛 T14, T15](https://ti.luogu.com.cn/problemset/1022)

[NOIP2016 换教室](https://uoj.ac/problem/262)（概率期望 DP）
